\documentclass{beamer}
\usetheme{Rochester}

\title[Composing like a human]{Composing like a human}
\subtitle{Adapting generative networks to few-shot learning in the musical domain}
\author{Tudor Paisa  \texorpdfstring{\\ (SNR: 2019551, ANR: 315146)}{} \texorpdfstring{\\ \texttt{t.paisa@tilburguniversity.edu}}{} }
\institute{Tilburg University}
\date{\today}

\begin{document}
\begin{frame}
    \titlepage
\end{frame}

%\begin{frame}
    %\frametitle{Outline}
    %\tableofcontents
%\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \section{Introduction}
    \begin{block}{Deep Learning}
    \begin{itemize}
        \item Deep Learning promises to discover rich hierarchical models over various applications
        \item The majority of developments usually involve discriminative models
        \item In recent years...
            \begin{itemize}
            \item Generative Models
            \item Few-Shot Learning
            \end{itemize}
    \end{itemize}
\end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{block}{Generative Models}
        \begin{itemize}
            \item A Neural Network that is able to generate data
            \item Simple concept...
                \begin{itemize}
                    \item ... slow developments...
                \end{itemize}
            \item ... until frameworks like GANs \& LSTM
            \item Widely researched in Computer Vision, Natural Language Processing, not so much in arts (music especially)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{block}{Few-Shot Learning}
        \begin{itemize}
            \item Neural Networks tend to be "data-hungry"
            \item SGD does not allow convergence in a limited number of steps
            \item For AI applications this sucks
                \begin{itemize}
                    \item Humans generalize after one/few examples
                    \item Vanilla neural nets do not
                \end{itemize}
            \item Few-Shot Learning: making neural networks generalize using by showing it few instances of each class
            \item Solutions? Yes! More on that later
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Introduction}
    \begin{block}{Getting There...}
        \begin{itemize}
            \item Overwhelming majority of few-shot generative developments have been in Computer Vision
            \item Then some in Natural Language Processing
            \item Musical domain largely unexplored
        \end{itemize}
    \end{block}

    \begin{block}{The research!}
        \begin{itemize}
            \item Adapt two generative models to few-shot learning
            \item Evaluate their ability to create music
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}
    \frametitle{Theoretical Framework}
    \begin{block}{Music Generators}
        There are two candidates:
    \end{block}
        \begin{block}{C-RNN-GAN}
            \begin{minipage}[b]{0.48\textwidth}
                $G$enerator \\
                2 $\times$ \textbf{unidirectional} LSTM \\
                350 hidden units
            \end{minipage}
            \hfill
            \begin{minipage}[b]{0.48\textwidth}
                $D$iscriminator \\
                2 $\times$ \textbf{bidirectional} LSTM \\
                350 hidden units
            \end{minipage}
        \end{block}
        \begin{block}{PerformanceRNN}
            3 $\times$ \textbf{unidirectional} LSTM \\
            512 hidden units
        \end{block}
\end{frame}

\end{document}
