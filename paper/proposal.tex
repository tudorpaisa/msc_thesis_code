\documentclass[11pt, a4paper, man, floatsintext]{apa6}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[ruled]{algorithm2e}

\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\addbibresource{references.bib}

\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{subcaption}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Composing like a human: Adapting generative networks to few-shot learning in the musical domain}
\shorttitle{Composing like a human}

\author{Tudor Paisa\\
        SNR: 2019551 | ANR: 315146\\
        t.paisa@tilburguniversity.edu
    }

\note{Thesis Supervisor:\\ Dr. Menno van Zaanen}

\affiliation{Tilburg University\\
        School of Humanities and Digital Sciences\\
        Department of Cognitive Science \& Artificial Intelligence\\
        Tilburg, The Netherlands}

\authornote{Thesis proposal submitted in partial fulfillment of the requirements for the degree of Master of Science in Data Science and Society, at the School of Humanities and Digital Sciences of Tilburg University}

% Typography
\usepackage{times}

% Links
\usepackage[colorlinks=true]{hyperref}

\begin{document}

\maketitle

\section{Introduction}

The reemergence of neural networks (NN) has brought new ways to tackle Machine Learning problems (see \cite{lecun_deep_2015} for examples), as well as new possibilities to further artificial intelligence research (such as general-purpose AI systems that play video-games; \cite{vinyals_starcraft_2017} respectively). Currently, two exciting areas of research in the field of Deep Learning (DL) are in designing generative models \parencite{goodfellow_nips_2016} and training NNs on small datasets \parencite{finn_model-agnostic_2017, nichol_first-order_2018}. This study will combine both tasks by exploring the extent to which generative models can create novel and qualitative samples of music under the constraints posed by the few-shot learning problem. In other words, when data is scarce, can a generative model create samples of music that are new (i.e., not in the training set), consistent with common scales, and diverse (tone sequences are not repeated over and over, combines chord structures with single-note sequences, is not restricted to - say - one octave). This research will evaluate the generated samples of two generative models (C-RNN-GAN of \cite{mogren_c-rnn-gan_2016} and Performance-RNN of \cite{oore_this_2018}) that have been trained under the Reptile learning framework \parencite{nichol_first-order_2018} on small amounts of MIDI data.

DL as a field, promises to discover rich hierarchical models \parencite{lecun_deep_2015}, over various artificial intelligence applications, such as speech recognition, image classification, or text generation. The majority of developments usually involved discriminative models \parencite{goodfellow_generative_2014}, where high-dimensional input is assigned to a class label \parencite[e.g.,][]{krizhevsky_imagenet_2012}. In contrast, successes in generative models have been scarce until the discovery of the Generative Adversarial Network (GAN) by \textcite{goodfellow_generative_2014}. GANs have had significant successes in image super-resolution \parencite{ledig_photo-realistic_2016}, style-based image generation \parencite{karras_style-based_2018}, or image-to-image translation \parencite[such as a sketch into a photorealistic image;][]{isola_image--image_2016}. These developments lie in the field of Computer Vision, but that is not to say that we can generate only images. GANs (and others) have been applied in Natural Language Processing where the scope was to generate high-quality text with sufficient diversity \parencite[e.g.,][]{yu_seqgan_2016, chen_adversarial_2018}. Less explored, but still successful, have been the efforts in generating music \parencite[e.g.,][]{mogren_c-rnn-gan_2016, dong_musegan_2017}. This paper will focus on the problem of generating music with the models proposed by \textcite{mogren_c-rnn-gan_2016} and \textcite{oore_this_2018}, trained under the few-shot learning problem (detailed below).

It is fairly well known that DL models tend to be "data-hungry", a caveat imposed by its optimization algorithm \parencite{chen_closer_2018, ravi_optimization_2016}. DL usually relies on a procedure called stochastic gradient descent (SGC) which implies computing for a given input vector the output and error, followed by calculating the input's average gradient and consequently adjusting the network's weights \parencite{lecun_deep_2015}. \textcite{ravi_optimization_2016} argue that the iterative nature of gradient-optimization algorithms does not allow them to perform well under the constraint of a set number of updates. Finally, they add by saying that for each new problem (e.g., new dataset), the model needs to reinitialize with a new set of random parameters which hurts the network's ability to converge to a good solution when constrained by a limited number of updates \parencite{ravi_optimization_2016}.

Besides the promise of making DL algorithms easier to train, primary reasons why this issue needs to be addressed are as follows. Firstly, humans have the ability to generalize after one (or few) example(s) of a given object \parencite{vinyals_matching_2016, ravi_optimization_2016}, something which DL algorithms do not. Secondly, this would alleviate the constant need for data collection to ensure reasonable performances. Finally, there are many fields where the data exhibits a large number of classes and few examples per class; approaching human-type learning could allow models to properly capture this sort of data \parencite{ravi_optimization_2016, larochelle_few-shot_2017}. This is known as the few-shot problem \parencite{vinyals_matching_2016}.

In spite of its proposed benefits, developments and experiments in few-shot learning are still scarce \parencite{larochelle_few-shot_2017} and evaluations are largely concentrated on image data \parencite[e.g.,][]{lake_omniglot_2019, clouatre_figr_2019, chen_closer_2018}. Few-shot experiments with generative networks are even less frequent \parencite{zhang_metagan_2018, clouatre_figr_2019}. This research aims to explore the viability of this learning approach in the context of generating music data. More concretely, in light of recent developments, I will evaluate two generative networks trained on MIDI data under the constraints of few-shot learning, and compare their performance to the baseline proposed by \textcite{larochelle_few-shot_2017} and properties extracted from real music performances.

\section{Related Work}

\textcite{lake_human-level_2015} popularized the idea of learning from a few class examples using a probabilistic learning framework capable of generalizing and learning a large number of concepts from a single class example (i.e., one-shot learning). \textcite{rezende_one-shot_2016} were among the first to provide a solution to the one-shot problem in the context of a neural network (NN) by implementing Bayesian reasoning into a deep NN embedded within hierarchical latent variable models. The Reptile algorithm \parencite{nichol_first-order_2018} is a recent approach to this problem. It counteracts the shortcomings of gradient-optimization algorithms by learning the model's initial parameters in order to maximize its performance on novel tasks. Furthermore, it allows for any type of network to be used and does not place any restrictions on the type of loss function that can be used \parencite{nichol_first-order_2018}. Moreover, it avoids the computational slowdowns of its counterpart, MAML \parencite{finn_model-agnostic_2017}. The algorithm for Reptile is as follows:

\IncMargin{1em}
\begin{algorithm}
Initialize $\phi$, the vector of initial parameters

\For{iteration = $1, 2, \dots$}{
    Sample task $\tau$, corresponding to loss $L_{\tau}$ on weight vectors $\tilde \phi$

    Compute $\tilde \phi = U_{\tau}^{k} (\phi)$, denoting $k$ steps of SGD or Adam

    Update $\phi \leftarrow \phi + \epsilon (\tilde{\phi} - \phi)$
}

\caption{Reptile (serial version)}
\label{alg:reptile}
\end{algorithm}
\DecMargin{1em}

Turning over to the generative side of things, a popular form of generating data is through the use of a Generative Adversarial Network \parencite[GAN;][]{goodfellow_generative_2014}. Here a generative model $G$ captures the data distribution and creates samples from it, whereas a discriminative model $D$ estimates the probability that the input is real (comes from the training data) rather than fake (comes from $G$). Formally, given $x$ sampled from real data, and $z$ being input noise, $D$ tries to keep $D(x)$ near $1$, while making $D(G(z))$ near $0$. Conversely, $G$ tries to make $D(G(z))$ near $1$ \parencite{goodfellow_nips_2016}. A notable implementation is C-RNN-GAN \parencite{mogren_c-rnn-gan_2016}, a continuous recurrent neural network with adversarial training. It consists of a network with two LSTM layers and 350 hidden units per layer, in both $G$ and $D$. However, $D$ has a bidirectional layout which allows it to take context from both past and future for its decisions, while $G$ is unidirectional \parencite{mogren_c-rnn-gan_2016}.

Notwithstanding GANs, other generative successes have made use of adaptations on the Long Short-Term Memory (LSTM) network \parencite{hochreiter_long_1997}. Here, the cells of a network are designed to allow for information to persist by implementing a recursive state unit, an input gate, a forget gate, and an output gate. The input of the network is allowed into the state unit if it is allowed to pass through by the input gate. Next, the forget gate controls the self-loop of the state cell. Finally, the output gate has the power to shut off the output cell \parencite{goodfellow_deep_2016}. Here, a notable implementation is Performance-RNN \parencite{oore_this_2018}; a three layer unidirectional LSTM network with 512 units each.

GANs and LSTM networks have been tested and adapted to the few-shot learning problem \parencite[see][]{zhang_metagan_2018, vinyals_matching_2016} however, no such evaluation has been made in the musical domain. Therefore, this study will seek to answer the following research questions:

\begin{itemize}
    \item To what extent is the music created by a few-shot generative model comparable to the music of a generative model that is trained on the entire dataset?
    \item To what extent is the music created by a few-shot generative model comparable to real music?
\end{itemize}

\section{Methods}

To provide an answer to the above research questions, this study will develop (in Keras) two generative models based on C-RNN-GAN and Performance-RNN, where the learning procedure will incorporate the Reptile algorithm. Other than that, the models will not differ from their original implementation. These models will firstly be compared to a baseline (a LSTM network of depth 1 and 200 units trained on the entire train set) and then with the music from the dataset. The evaluation between the models with the baseline and with the music in the dataset will be based on the number of statistically different bins \parencite[NDB;][]{richardson_gans_2018} and domain specific measurements (polyphony, scale consistency, repetitions, tone span). Refer to the evaluation section for more details on these metrics. The data for the models will be one-hot encoded as per \textcite{oore_this_2018}; a vector for each time slice containing Note-On (128 values), Note-Off (128 values), Time-Shift (125 values), and Velocity (32 values) events.

\subsection{Dataset}

The dataset for this research will be similar to that of \textcite{oore_this_2018} namely, the MAESTRO Dataset \parencite{hawthorne_enabling_2018}. The dataset contains MIDI recordings from nine years of the International Piano-e-Competition. This amounts to 1,184 piano performances, approximately 430 compositions, 6.18 million notes played, and approximately 172 hours of playback. The dataset also features a recommended train/validation/test split. Moreover, each performance comes with additional metadata namely, the name of the composer, the title of the performance, the suggested train/validation/test split, year of the performance, name of the file, and duration in seconds of the performance. The name of the performer is not included \parencite{hawthorne_enabling_2018}.

\subsection{Evaluation}

Most DL models work by the principle of likelihood maximization, which simply says to choose the parameters that maximize the probability that the model assigns to the training data \parencite{goodfellow_nips_2016}. Formally, this means selecting the parameters that maximize $\sum_{i=1}^N \log p_{\text{model}}(\bm{x}^{(i)}; \bm{\theta})$:

\begin{equation}
    \bm{\theta^*} = \arg \max \sum_{i=1}^N \log p_{\text{model}}(\bm{x}^{(i)}; \bm{\theta}) \label{eg:log_mle}.
\end{equation}

That being said, calculating maximum likelihood can be thought of minimizing the Kullback-Leibler (KL) divergence. Thus, $\bm{\theta^*} = \argmin_{\bm{\theta}} \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} [\log p_{\text{data}} (\bm{x}) - \log p_{\text{model}} (\bm{x})]$. With this in mind, $\log p_{\text{data}}$ is a result of the data-generating process, and not the model. Therefore, a final simplification is applied where the maximum likelihood estimate would be calculated as:

\begin{equation}
    \bm{\theta^*} = - \mathbb{E}_{\mathbf{x} \sim p_{\text{data}}} [\log p_{\text{model}} (\bm{x})] \label{eq:nll}.
\end{equation}

Note that Eq. \ref{eg:log_mle} is the same as Eq. \ref{eq:nll}. The reason why this is brought up, is because negative log-likelihood (NLL; Eq. \ref{eq:nll}) is used both as a loss function and as an evaluation metric \parencite[][]{yu_seqgan_2016, borji_pros_2018}.  However, as \textcite{borji_pros_2018} notes, NLL is uninformative about the quality of the samples generated and it does not allow to answer whether the generative network is simply memorizing training examples. As such, this study will also include other evaluation metrics when assessing generative models: the number of statistically different bins \parencite[NDB;][]{richardson_gans_2018}, polyphony, scale consistency, repetitions, and tone span \parencite{mogren_c-rnn-gan_2016}.

NDB is an evaluation metric specifically designed for generative models. It follows the intuition that given two samples from the same distribution, the number of samples that fall into a bin (read as: cluster) should be the same \parencite{richardson_gans_2018}. Let $I_B(\mathbf{x})$ be the indicator function for bin $B$. Then $I_B(\mathbf{x}) = 1$ if the sample falls into $B$, and zero otherwise. Let also $\{\mathbf{x}_i^p\}$ define $N_p$ samples from a $p$ distribution (e.g., test set samples) and $\{\mathbf{x}_i^q\}$ be the $N_q$ samples from a $q$ distribution (e.g., generated samples). If $p = q$ then the followings is also true:

\begin{equation}
    \frac{1}{N_p} \sum_i I_B(\mathbf{x}_i^p) \approx \frac{1}{N_q} \sum_i I_B(\mathbf{x}_i^q)
\end{equation}

The pooled sample proportion $P_B$ is the proportion of samples (from the joined sets) that fall into $B$. The standard error for bin $B$ is given by:

\begin{equation}
    SE_B = \sqrt{P_B (1 - P_B)[1 / N_p + 1 / N_q]}
\end{equation}

The test statistic is a $z$-score $z = \frac{P_B^p - P_B^q}{SE_B}$ where $P_B^p$ and $P_B^q$ are the proportions of each sample that fall into $B$. If $z$ is smaller than a significance level, then the samples within that bin are statistically different. This process is repeated for each bin, and the number of statistically different bins is reported. The selection of bins is done through $K$-means clustering ($K \ll N_p, N_q$) on the $\mathbf{x}^p$ samples. Each of the generated samples $\mathbf{x}^q$ is by assigned to the nearest ($L_2$) $K$ centroid \parencite{richardson_gans_2018}.

The more domain-specific evaluation measures are adapted from \textcite{mogren_c-rnn-gan_2016}. Polyphony measures how often a minimum of two tones are played simultaneously (when the start time is the same). Scale consistency is the fraction of notes that are part of a standard scale (e.g., major, minor, lydian, etc.). Repetitions counts consecutive subsequences of notes, and tone span is the difference in semitones between the lowest note and highest note \parencite{mogren_c-rnn-gan_2016}.

The choice of NDB comes from the fact that it is good at detecting overfitting \parencite{borji_pros_2018}, whereas the domain-specific metrics would favor models that generate high fidelity samples. These evaluation metrics also have the benefit of being model-agnostic; they do not require a specific generative model.

\section{Milestones}

\begin{table}
    \centering
    \begin{tabular}{l|r}
        Task    & Date \\ \hline
        Develop one-hot encoder for MIDI data & March 15th \\
        Develop domain-specific evaluation tools & March 22nd \\
        Create a Keras implementation of Performance-RNN & March 24th \\
        Create baseline & March 24th \\
        Create a Keras implementation of C-RNN-GAN & March 29th \\
        Augment models with Reptile & April 5th \\
        Train models & April 5th - April 19th \\
        Evaluate models & April 26th \\
        Write thesis & April 26th - May 13th \\
    \end{tabular}
\end{table}


\printbibliography
\end{document}
